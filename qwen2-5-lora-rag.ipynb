{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"private_outputs":true,"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11772920,"sourceType":"datasetVersion","datasetId":7382479}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/shakil19/qwen2-5-lora-rag?scriptVersionId=239146859\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!nvidia-smi","metadata":{"id":"ZqwAEhxm_viS","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os","metadata":{"id":"5iYqpgcd_y5F","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # %%capture\n# import os\n# if \"COLAB_\" not in \"\".join(os.environ.keys()):\n#     !pip install unsloth\n# else:\n#     # Do this only in Colab notebooks! Otherwise use pip install unsloth\n# !uv pip install --system --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo -qq\n# !uv pip install --system sentencepiece protobuf datasets huggingface_hub hf_transfer -qq\n# !uv pip install --system --no-deps unsloth","metadata":{"id":"bs6Lowaw_zxC","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!uv pip install --system unsloth vllm -qq","metadata":{"id":"0VXlUfZy_4Xi","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from google.colab import userdata\n# HF = userdata.get('HF_TOKEN')","metadata":{"id":"DcHsDc5P_7nK","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nHF = user_secrets.get_secret(\"HF_TOKEN\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch, os\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# HF = os.getenv(\"HF_TOKEN\")\nmax_seq_length = 1024\nlora_rank = 64\n\n# Re-run the code after restarting the kernel\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"Qwen/Qwen2.5-3B-Instruct\",\n    max_seq_length = max_seq_length,\n    load_in_4bit = True, # False for LoRA 16bit\n    fast_inference = True, # Enable vLLM fast inference\n    max_lora_rank = lora_rank,\n    gpu_memory_utilization = 0.5, # Reduce if out of memory\n)","metadata":{"id":"f64EZDszAGay","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r = 32,\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 32,\n    lora_dropout = 0,\n    bias = \"none\",\n    use_gradient_checkpointing = \"unsloth\",\n    use_rslora = False,\n    loftq_config = None,\n)","metadata":{"id":"xWZa1NF9AJ_L","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset, concatenate_datasets\nreasoning_dataset = load_dataset(\"json\", data_files=\"/kaggle/input/may-10/intent_dataset.json\", split=\"train\")\nnon_reasoning_dataset = load_dataset(\"json\", data_files=\"/kaggle/input/may-10/qna.json\", split=\"train\")","metadata":{"id":"B0-A0rfUmrNv","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pprint import pprint\npprint(reasoning_dataset[0])\npprint(non_reasoning_dataset[0])","metadata":{"id":"pObfR5F_muYt","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_conversation(examples):\n    # Access the correct column names from reasoning_dataset\n    problems = examples[\"patterns\"]\n    solutions = examples[\"responses\"]\n    conversations = []\n    for problem, solution in zip(problems, solutions):\n        # Convert problem and solution to strings if they are lists\n        problem = problem if isinstance(problem, str) else ' '.join(problem)\n        solution = solution if isinstance(solution, str) else ' '.join(solution)\n\n        conversations.append([\n            {\"role\": \"user\", \"content\": problem},\n            {\"role\": \"bot\", \"content\": solution},\n        ])\n    return {\"conversations\": conversations}","metadata":{"id":"5qvsNKEPnCqw","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"reasoning_conversations = tokenizer.apply_chat_template(\n    reasoning_dataset.map(generate_conversation, batched = True)[\"conversations\"],\n    tokenize = False,\n)\nreasoning_conversations[0]","metadata":{"id":"mq32yKYqnDiV","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from unsloth.chat_templates import standardize_sharegpt\n\n# Prepare the dataset\ndataset = standardize_sharegpt(non_reasoning_dataset)\n\n# Create the \"conversations\" column from \"user\" and \"bot\" for each example\ndef create_conversation(example):\n    # The chat template expects [{'role':'user', ...}, {'role':'assistant','...'}] (assistant not 'bot')\n    return {\n        \"conversations\": [\n            {\"role\": \"user\", \"content\": example[\"user\"]},\n            {\"role\": \"assistant\", \"content\": example[\"bot\"]},\n        ]\n    }\n\ndataset = dataset.map(create_conversation)","metadata":{"id":"MalgYTUjnQ9D","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Apply the chat template and store the **prompt** in a new column\ndef apply_template(example):\n    prompt = tokenizer.apply_chat_template(\n        example[\"conversations\"],\n        tokenize=False\n    )\n    return {\"prompt\": prompt}\n\ndataset = dataset.map(apply_template)","metadata":{"id":"xguZYFvinRxF","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"non_reasoning_conversations = dataset[\"prompt\"]\npprint(non_reasoning_conversations[0])\npprint(len(reasoning_conversations))\npprint(len(non_reasoning_conversations))","metadata":{"id":"CdqSxIUBneY-","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"chat_percentage = 0.75","metadata":{"id":"hFAFv2nwnfva","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nnon_reasoning_subset = pd.Series(non_reasoning_conversations)\nnon_reasoning_subset = non_reasoning_subset.sample(\n    int(len(reasoning_conversations) * (1.0 - chat_percentage)),\n    random_state = 2407,\n)","metadata":{"id":"ZDja05pNnjs1","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = pd.concat([\n    pd.Series(reasoning_conversations),\n    pd.Series(non_reasoning_subset)\n])\ndata.name = \"text\"","metadata":{"id":"XISjB-G9nrRY","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import Dataset\ncombined_dataset = Dataset.from_pandas(pd.DataFrame(data))\ncombined_dataset = combined_dataset.shuffle(seed = 3407)\n\nlen(combined_dataset)","metadata":{"id":"NYOxJWDjnscN","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from trl import SFTTrainer, SFTConfig\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = combined_dataset,\n    eval_dataset = None,\n    args = SFTConfig(\n        dataset_text_field = \"text\",\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 4,\n        warmup_steps = 5,\n        max_steps = 30,\n        learning_rate = 2e-4,\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        report_to = \"none\",\n    )\n)","metadata":{"id":"UHeBYNZdnyVj","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"id":"QlZizpn2n4DW","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"messages = [\n    {\"role\" : \"user\", \"content\" : \"What do you know about the sun?\"},\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize = False,\n    add_generation_prompt = True,\n    enable_thinking = False,\n)","metadata":{"id":"XrnW8gF8oDuQ","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import TextStreamer\n_ = model.generate(\n    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n    max_new_tokens = 2048,\n    temperature = 0.7, top_p = 0.8, top_k = 20,\n    streamer = TextStreamer(tokenizer, skip_prompt = True),\n)","metadata":{"id":"dBA4nD8Sn4bl","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"messages = [\n    {\"role\" : \"user\", \"content\" : \"What is a latte?\"}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize = False,\n    add_generation_prompt = True,\n    enable_thinking = True,\n)\n\nfrom transformers import TextStreamer\n_ = model.generate(\n    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n    max_new_tokens = 2048,\n    temperature = 0.7, top_p = 0.8, top_k = 20,\n    streamer = TextStreamer(tokenizer, skip_prompt = True)\n)","metadata":{"id":"jGRiedDCoKN0","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_pretrained(\"vllm-out\")\ntokenizer.save_pretrained(\"vllm-out\")","metadata":{"id":"Ff4abzNOoO4p","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_pretrained_merged(\"vllm-out\", tokenizer, save_method=\"merged_16bit\")\nprint(f\"Merged 16-bit model saved locally to: vllm-out/\")","metadata":{"id":"LbrQW_Kzpkv6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pprint(type(model))","metadata":{"id":"DojR5JKHtd0R","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for name, param in model.named_parameters():\n    print(name, param.shape)","metadata":{"id":"Atyny-lNplR7","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.push_to_hub_merged(\"Moitreyee444/qwen2.5_3B_lora_model\", tokenizer, save_method = \"merged_16bit\", token = HF)","metadata":{"id":"RpExI9fQtcBQ","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## RAG Pipeline","metadata":{"id":"hQSf6llmwJrY"}},{"cell_type":"code","source":"import os\nfrom vllm import LLM, SamplingParams\n# from google.colab import userdata\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n# https://huggingface.co/Moitreyee444/qwen2.5_3B_lora_model\nhub_model_id = \"shakil-mosharrof/qwen2.5_lora_model\"\n\nllm = LLM(\n    model=hub_model_id,\n    trust_remote_code=True,  # Important for Qwen models\n)\n\nsampling_params = SamplingParams(temperature=0.7, top_p=0.95, max_tokens=256)\n\n# Generate text\nprompts = [\"Explain the theory of relativity in simple terms:\", \"What is the recipe for a good chocolate cake?\"]\noutputs = llm.generate(prompts, sampling_params)\n\nfor output in outputs:\n    prompt = output.prompt\n    generated_text = output.outputs[0].text\n    print(f\"Prompt: {prompt!r}, Generated: {generated_text!r}\")","metadata":{"id":"VmV_JCnKwXed","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pprint import pprint\n# Generate text\nprompts = [\"Explain the theory of relativity in simple terms:\", \"What is the recipe for a good chocolate cake?\"]\noutputs = llm.generate(prompts, sampling_params)\n\nfor output in outputs:\n    prompt = output.prompt\n    generated_text = output.outputs[0].text\n    pprint(f\"Prompt: {prompt!r}, Generated: {generated_text!r}\")","metadata":{"id":"WijBPaADzpQj","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!uv pip install --system streamlit chromadb langchain_community sentence-transformers pypdf transformers vllm -q","metadata":{"id":"W8CBsfbTwZKt","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import chromadb\nfrom chromadb.utils import embedding_functions\nfrom sentence_transformers import SentenceTransformer\nfrom pathlib import Path\nimport os\nfrom langchain_community.document_loaders import PyPDFLoader\n# Imports for vLLM and Hugging Face Tokenizer\nfrom vllm import LLM, SamplingParams\nfrom transformers import AutoTokenizer\n\n\nFINETUNED_MODEL_HUB_ID = \"shakil-mosharrof/qwen2.5_lora_model\" # <<< IMPORTANT: VERIFY THIS\n\n# --- 1. Prepare Your Corpus ---\nloader = PyPDFLoader('/kaggle/input/may-10/sec-3.pdf')\ndocuments = loader.load()\n\n# --- 2. Initialize Embedding Model for ChromaDB ---\n\nretrieval_embedder_model_name = 'all-MiniLM-L6-v2'\n\n\n# --- 3. Initialize ChromaDB Persistently ---\nchroma_client = chromadb.PersistentClient(path=\"chroma_rag_db_vllm\")\ncollection = chroma_client.get_or_create_collection(\n    name=\"knowledge_vllm\",\n    embedding_function=embedding_functions.SentenceTransformerEmbeddingFunction(\n        model_name=retrieval_embedder_model_name\n    )\n)\n\n# --- 4. Chunk and Store: Add to ChromaDB (if not already done) ---\n# Check if collection is empty before adding to avoid duplicates on re-runs\nif collection.count() == 0:\n    print(f\"Adding {len(documents)} documents to ChromaDB collection '{collection.name}'...\")\n    texts = [doc.page_content for doc in documents]\n    \n    collection.add(\n        documents=texts,\n        metadatas=[{\"source\": f\"doc_{i}\"} for i in range(len(texts))],\n        ids=[f\"id_{i}\" for i in range(len(texts))]\n    )\n    print(\"Documents added to ChromaDB.\")\nelse:\n    print(f\"ChromaDB collection '{collection.name}' already contains {collection.count()} documents.\")\n\n# --- 5. Initialize vLLM and Tokenizer for the Fine-tuned Model ---\nprint(f\"Loading fine-tuned model '{FINETUNED_MODEL_HUB_ID}' with vLLM...\")\n\ntry:\n    # Load tokenizer for the fine-tuned Qwen2 model\n    # This is crucial for applying the correct chat template\n    qwen_tokenizer = AutoTokenizer.from_pretrained(\n        FINETUNED_MODEL_HUB_ID,\n        trust_remote_code=True,\n    )\n\n    # Initialize vLLM with the fine-tuned model\n    llm = LLM(\n        model=FINETUNED_MODEL_HUB_ID,\n        trust_remote_code=True,\n    )\n    print(\"Fine-tuned model and tokenizer loaded successfully.\")\nexcept Exception as e:\n    print(f\"Error loading fine-tuned model or tokenizer: {e}\")\n    print(\"Please ensure:\")\n    print(f\"1. '{FINETUNED_MODEL_HUB_ID}' is the correct Hugging Face Hub ID.\")\n    print(\"2. Your Hugging Face token (from 'HF' env var) is valid and has permissions if the model is private.\")\n    print(\"3. You have vLLM and necessary dependencies (like PyTorch with CUDA) installed correctly.\")\n    exit()\n\n# Define sampling parameters for vLLM\n# Note: `max_new_tokens` from HF becomes `max_tokens` in vLLM SamplingParams\nsampling_params = SamplingParams(\n    temperature=0.7,\n    top_p=0.8,\n    top_k=20,\n    max_tokens=512  # Max number of tokens to generate\n)\n\n# --- 6. Set up System Prompt and User Query Loop ---\nsystem_prompt = f\"\"\"\nYou are an AI-powered expert assistant specialized in extracting and \ninterpreting policies and regulations of the Bangladesh Air Force from provided PDF documents. \nYour task is to accurately and efficiently understand dense, complex legal and procedural texts, and provide clear, precise, \nand contextually relevant answers to user queries about Air Force policies and regulations. When responding:  \n\n- Base answers strictly on the content of the given documents.  \n- Explain policy or regulation details in simple, clear language suitable for users at different hierarchy levels.  \n- Provide specific references or excerpts from the source documents when helpful.  \n- Handle complex queries by breaking down information into understandable parts.  \n- Continuously learn and improve from interactions to better support decision-making and compliance processes.  \n\nMaintain a professional and concise tone, prioritizing accuracy and usefulness for users navigating Air Force policies and regulations.\n\n\"\"\"\n\nprint(\"\\n--- RAG Chat with vLLM ---\")\nprint(\"Type 'exit' or 'quit' to end.\")\n\nwhile True:\n    user_query = input(\"USER: \")\n    if user_query.strip().lower() in {\"exit\", \"quit\"}:\n        break\n    if not user_query.strip():\n        continue\n\n    # --- 6a. Retrieve Relevant Chunks from ChromaDB ---\n    results = collection.query(\n        query_texts=[user_query],\n        n_results=3  # Retrieve top 3 relevant chunks\n    )\n    contexts = [doc for doc in results[\"documents\"][0]] if results[\"documents\"] and results[\"documents\"][0] else []\n\n    if not contexts:\n        retrieved_context_str = \"No relevant context found in the database.\"\n    else:\n        retrieved_context_str = \"\\n\".join([f\"- {ctx}\" for ctx in contexts])\n\n    # --- 7. Compose Prompt for Qwen2 Model ---\n    messages = [\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"system\", \"content\": \"Context:\\n\" + retrieved_context_str},\n        {\"role\": \"user\", \"content\": user_query},\n    ]\n\n    # Apply the chat template using the Qwen2 tokenizer\n    # This creates the full prompt string with special tokens for the model\n    try:\n        full_prompt_string = qwen_tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True,\n        )\n    except Exception as e:\n        print(f\"Error applying chat template: {e}\")\n        print(\"This might happen if the tokenizer doesn't support `enable_thinking` or if the messages format is unexpected.\")\n        print(\"Try removing `enable_thinking=False` if you are not using Unsloth or a custom template that requires it.\")\n        continue\n\n\n    # --- 8. Generate Response with vLLM ---\n    print(\"ASSISTANT: \", end=\"\", flush=True)\n    try:\n        vllm_outputs = llm.generate([full_prompt_string], sampling_params)\n\n        # Extract the generated text\n        generated_text = vllm_outputs[0].outputs[0].text\n        print(generated_text)\n\n    except Exception as e:\n        print(f\"\\nError during vLLM generation: {e}\")\n\n    print(\"-\" * 20) # Separator for conversation turns\n\nprint(\"Exiting RAG pipeline.\")","metadata":{"id":"e3Ech4fQt39A","trusted":true},"outputs":[],"execution_count":null}]}